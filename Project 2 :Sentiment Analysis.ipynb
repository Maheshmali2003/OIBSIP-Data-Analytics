{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78835a7c",
   "metadata": {},
   "source": [
    "# Project 2 :Customer Segmentation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f3613f",
   "metadata": {},
   "source": [
    "# Description:\n",
    "The primary goal is to develop a sentiment analysis model that can accurately classify the\n",
    "sentiment of text data, providing valuable insights into public opinion, customer feedback, and\n",
    "social media trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "006be3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f9e18e",
   "metadata": {},
   "source": [
    "# Key Concepts and Challenges:\n",
    "Sentiment Analysis: Analyzing text data to determine the emotional tone, whether positive,\n",
    "negative, or neutral.\n",
    "\n",
    "1.Natural Language Processing (NLP): Utilizing algorithms and models to understand and\n",
    "process human language.\n",
    "\n",
    "2.Machine Learning Algorithms: Implementing models for sentiment classification, such as\n",
    "Support Vector Machines, Naive Bayes, or deep learning architectures.\n",
    "\n",
    "3.Feature Engineering: Identifying and extracting relevant features from text data to enhance\n",
    "model performance.\n",
    "\n",
    "4.Data Visualization: Presenting sentiment analysis results through effective visualizations for\n",
    "clear interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3306ce06",
   "metadata": {},
   "source": [
    "# 1.Natural Language Processing (NLP): Utilizing algorithms and models to understand and process human language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d133b459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when modi promised “minimum government maximum...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk all the nonsense and continue all the dra...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what did just say vote for modi  welcome bjp t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asking his supporters prefix chowkidar their n...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer who among these the most powerful world...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kiya tho refresh maarkefir comment karo</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>surat women perform yagna seeks divine grace f...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>this comes from cabinet which has scholars lik...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>with upcoming election india saga going import...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gandhi was gay does modi</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  category\n",
       "0  when modi promised “minimum government maximum...      -1.0\n",
       "1  talk all the nonsense and continue all the dra...       0.0\n",
       "2  what did just say vote for modi  welcome bjp t...       1.0\n",
       "3  asking his supporters prefix chowkidar their n...       1.0\n",
       "4  answer who among these the most powerful world...       1.0\n",
       "5           kiya tho refresh maarkefir comment karo        0.0\n",
       "6  surat women perform yagna seeks divine grace f...       0.0\n",
       "7  this comes from cabinet which has scholars lik...       0.0\n",
       "8  with upcoming election india saga going import...       1.0\n",
       "9                         gandhi was gay does modi         1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"Twitter_Data.csv\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c79e5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03070aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tweepy \n",
    "from tweepy import OAuthHandler\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7c097f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error :  403 Forbidden\n",
      "453 - You currently have access to a subset of Twitter API v2 endpoints and limited v1.1 endpoints (e.g. media post, oauth) only. If you need access to this endpoint, you may need a different access level. You can learn more here: https://developer.twitter.com/en/portal/product\n",
      "Failed to fetch tweets. Exiting...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "from textblob import TextBlob\n",
    "\n",
    "class TwitterClient(object):\n",
    "    '''\n",
    "    Generic Twitter Class for sentiment analysis.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Class constructor or initialization method.\n",
    "        '''\n",
    "        # keys and tokens from the Twitter Dev Console\n",
    "        consumer_key = 'VythiCYHrNh52G2fwb6LiNV6c'\n",
    "        consumer_secret = 'gxQGtGSlH89X2HiAz48gxPwCUcoAr8elbrHJr28g9D6Mfg8320'\n",
    "        access_token = '1770327110126915584-6uESH0HOsEoYBlBeT79KAmLLRPDrMB'\n",
    "        access_token_secret = '5FqBLqxfT2MZM9mygXRRCuqbZPcCmSOb1UTeZk0vyBlQc'\n",
    "\n",
    "\n",
    "        # attempt authentication\n",
    "        try:\n",
    "            # create OAuthHandler object\n",
    "            self.auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "            # set access token and secret\n",
    "            self.auth.set_access_token(access_token, access_token_secret)\n",
    "            # create tweepy API object to fetch tweets\n",
    "            self.api = tweepy.API(self.auth)\n",
    "        except Exception as e:\n",
    "            print(\"Error: Authentication Failed - \", str(e))\n",
    "\n",
    "    def clean_tweet(self, tweet):\n",
    "        '''\n",
    "        Utility function to clean tweet text by removing links, special characters\n",
    "        using simple regex statements.\n",
    "        '''\n",
    "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "\n",
    "    def get_tweet_sentiment(self, tweet):\n",
    "        '''\n",
    "        Utility function to classify sentiment of passed tweet\n",
    "        using textblob's sentiment method\n",
    "        '''\n",
    "        # create TextBlob object of passed tweet text\n",
    "        analysis = TextBlob(self.clean_tweet(tweet))\n",
    "        # set sentiment\n",
    "        if analysis.sentiment.polarity > 0:\n",
    "            return 'positive'\n",
    "        elif analysis.sentiment.polarity == 0:\n",
    "            return 'neutral'\n",
    "        else:\n",
    "            return 'negative'\n",
    "\n",
    "    def get_tweets(self, query, count=10):\n",
    "        '''\n",
    "        Main function to fetch tweets and parse them.\n",
    "        '''\n",
    "        # empty list to store parsed tweets\n",
    "        tweets = []\n",
    "\n",
    "        try:\n",
    "            # call twitter api to fetch tweets\n",
    "            fetched_tweets = self.api.search_tweets(q=query, count=count)  # Use search_tweets instead of search\n",
    "\n",
    "            # parsing tweets one by one\n",
    "            for tweet in fetched_tweets:\n",
    "                # empty dictionary to store required params of a tweet\n",
    "                parsed_tweet = {}\n",
    "\n",
    "                # saving text of tweet\n",
    "                parsed_tweet['text'] = tweet.text\n",
    "                # saving sentiment of tweet\n",
    "                parsed_tweet['sentiment'] = self.get_tweet_sentiment(tweet.text)\n",
    "\n",
    "                # appending parsed tweet to tweets list\n",
    "                if not tweet.retweeted and 'RT @' not in tweet.text:\n",
    "                    tweets.append(parsed_tweet)\n",
    "\n",
    "            # return parsed tweets\n",
    "            return tweets\n",
    "\n",
    "        except Exception as e:\n",
    "            # print error (if any)\n",
    "            print(\"Error : \", str(e))\n",
    "\n",
    "def main():\n",
    "    # creating object of TwitterClient Class\n",
    "    api = TwitterClient()\n",
    "    # calling function to get tweets\n",
    "    tweets = api.get_tweets(query='Donald Trump', count=200)\n",
    "    \n",
    "    # Check if tweets is None\n",
    "    if tweets is None:\n",
    "        print(\"Failed to fetch tweets. Exiting...\")\n",
    "        return\n",
    "\n",
    "    # picking positive tweets from tweets\n",
    "    ptweets = [tweet for tweet in tweets if tweet['sentiment'] == 'positive']\n",
    "    # percentage of positive tweets\n",
    "    if len(tweets) > 0:\n",
    "        print(\"Positive tweets percentage: {} %\".format(100 * len(ptweets) / len(tweets)))\n",
    "    else:\n",
    "        print(\"No tweets found.\")\n",
    "        \n",
    "    # picking negative tweets from tweets\n",
    "    ntweets = [tweet for tweet in tweets if tweet['sentiment'] == 'negative']\n",
    "    # percentage of negative tweets\n",
    "    if len(tweets) > 0:\n",
    "        print(\"Negative tweets percentage: {} %\".format(100 * len(ntweets) / len(tweets)))\n",
    "    else:\n",
    "        print(\"No tweets found.\")\n",
    "\n",
    "    # percentage of neutral tweets\n",
    "    if len(tweets) > 0:\n",
    "        print(\"Neutral tweets percentage: {} %\".format(100 * (len(tweets) - (len(ntweets) + len(ptweets))) / len(tweets)))\n",
    "    else:\n",
    "        print(\"No tweets found.\")\n",
    "\n",
    "    # printing first 5 positive tweets\n",
    "    print(\"\\n\\nPositive tweets:\")\n",
    "    for tweet in ptweets[:10]:\n",
    "        print(tweet['text'])\n",
    "\n",
    "    # printing first 5 negative tweets\n",
    "    print(\"\\n\\nNegative tweets:\")\n",
    "    for tweet in ntweets[:10]:\n",
    "        print(tweet['text'])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # calling main function\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe286b66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf11167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d94851f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'fascinating', '!', 'It', 'involves', 'analyzing', 'text', 'data', 'to', 'extract', 'meaningful', 'insights', '.']\n",
      "Tokens after stopword removal: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'fascinating', '!', 'involves', 'analyzing', 'text', 'data', 'extract', 'meaningful', 'insights', '.']\n",
      "Stemmed tokens: ['natur', 'languag', 'process', '(', 'nlp', ')', 'fascin', '!', 'involv', 'analyz', 'text', 'data', 'extract', 'meaning', 'insight', '.']\n",
      "Lemmatized tokens: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'fascinating', '!', 'involves', 'analyzing', 'text', 'data', 'extract', 'meaningful', 'insight', '.']\n",
      "POS tags: [('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('fascinating', 'NN'), ('!', '.'), ('involves', 'VBZ'), ('analyzing', 'VBG'), ('text', 'NN'), ('data', 'NNS'), ('extract', 'NN'), ('meaningful', 'JJ'), ('insights', 'NNS'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('words')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/words\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Asus/nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('words')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/words.zip/words/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Asus/nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOS tags:\u001b[39m\u001b[38;5;124m\"\u001b[39m, pos_tags)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Named Entity Recognition (NER)\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m ner_tags \u001b[38;5;241m=\u001b[39m ne_chunk(pos_tags)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNER tags:\u001b[39m\u001b[38;5;124m\"\u001b[39m, ner_tags)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\chunk\\__init__.py:184\u001b[0m, in \u001b[0;36mne_chunk\u001b[1;34m(tagged_tokens, binary)\u001b[0m\n\u001b[0;32m    182\u001b[0m     chunker_pickle \u001b[38;5;241m=\u001b[39m _MULTICLASS_NE_CHUNKER\n\u001b[0;32m    183\u001b[0m chunker \u001b[38;5;241m=\u001b[39m load(chunker_pickle)\n\u001b[1;32m--> 184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mparse(tagged_tokens)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\chunk\\named_entity.py:127\u001b[0m, in \u001b[0;36mNEChunkParser.parse\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens):\n\u001b[0;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;124;03m    Each token should be a pos-tagged word\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 127\u001b[0m     tagged \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tagger\u001b[38;5;241m.\u001b[39mtag(tokens)\n\u001b[0;32m    128\u001b[0m     tree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tagged_to_parse(tagged)\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tree\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tag\\sequential.py:61\u001b[0m, in \u001b[0;36mSequentialBackoffTagger.tag\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m     59\u001b[0m tags \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tokens)):\n\u001b[1;32m---> 61\u001b[0m     tags\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtag_one(tokens, i, tags))\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(tokens, tags))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tag\\sequential.py:81\u001b[0m, in \u001b[0;36mSequentialBackoffTagger.tag_one\u001b[1;34m(self, tokens, index, history)\u001b[0m\n\u001b[0;32m     79\u001b[0m tag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tagger \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_taggers:\n\u001b[1;32m---> 81\u001b[0m     tag \u001b[38;5;241m=\u001b[39m tagger\u001b[38;5;241m.\u001b[39mchoose_tag(tokens, index, history)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tag \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tag\\sequential.py:647\u001b[0m, in \u001b[0;36mClassifierBasedTagger.choose_tag\u001b[1;34m(self, tokens, index, history)\u001b[0m\n\u001b[0;32m    645\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchoose_tag\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens, index, history):\n\u001b[0;32m    646\u001b[0m     \u001b[38;5;66;03m# Use our feature detector to get the featureset.\u001b[39;00m\n\u001b[1;32m--> 647\u001b[0m     featureset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_detector(tokens, index, history)\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;66;03m# Use the classifier to pick a tag.  If a cutoff probability\u001b[39;00m\n\u001b[0;32m    650\u001b[0m     \u001b[38;5;66;03m# was specified, then check that the tag's probability is\u001b[39;00m\n\u001b[0;32m    651\u001b[0m     \u001b[38;5;66;03m# higher than that cutoff first; otherwise, return None.\u001b[39;00m\n\u001b[0;32m    652\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cutoff_prob \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tag\\sequential.py:694\u001b[0m, in \u001b[0;36mClassifierBasedTagger.feature_detector\u001b[1;34m(self, tokens, index, history)\u001b[0m\n\u001b[0;32m    684\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeature_detector\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens, index, history):\n\u001b[0;32m    685\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    686\u001b[0m \u001b[38;5;124;03m    Return the feature detector that this tagger uses to generate\u001b[39;00m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;124;03m    featuresets for its classifier.  The feature detector is a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[38;5;124;03m    See ``classifier()``\u001b[39;00m\n\u001b[0;32m    693\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 694\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_detector(tokens, index, history)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\chunk\\named_entity.py:101\u001b[0m, in \u001b[0;36mNEChunkParserTagger._feature_detector\u001b[1;34m(self, tokens, index, history)\u001b[0m\n\u001b[0;32m     90\u001b[0m     nextnextpos \u001b[38;5;241m=\u001b[39m tokens[index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# 89.6\u001b[39;00m\n\u001b[0;32m     93\u001b[0m features \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m: shape(word),\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwordlen\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(word),\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefix3\u001b[39m\u001b[38;5;124m\"\u001b[39m: word[:\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuffix3\u001b[39m\u001b[38;5;124m\"\u001b[39m: word[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m:]\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m\"\u001b[39m: pos,\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m\"\u001b[39m: word,\n\u001b[1;32m--> 101\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men-wordlist\u001b[39m\u001b[38;5;124m\"\u001b[39m: (word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_english_wordlist()),\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprevtag\u001b[39m\u001b[38;5;124m\"\u001b[39m: prevtag,\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprevpos\u001b[39m\u001b[38;5;124m\"\u001b[39m: prevpos,\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnextpos\u001b[39m\u001b[38;5;124m\"\u001b[39m: nextpos,\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprevword\u001b[39m\u001b[38;5;124m\"\u001b[39m: prevword,\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnextword\u001b[39m\u001b[38;5;124m\"\u001b[39m: nextword,\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword+nextpos\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m+\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnextpos\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpos+prevtag\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpos\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m+\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprevtag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape+prevtag\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprevshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m+\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprevtag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    110\u001b[0m }\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\chunk\\named_entity.py:52\u001b[0m, in \u001b[0;36mNEChunkParserTagger._english_wordlist\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m words\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_en_wordlist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(words\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men-basic\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     53\u001b[0m     wl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_en_wordlist\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wl\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__load()\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('words')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/words\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Asus/nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "# Sample text\n",
    "text = \"Natural Language Processing (NLP) is fascinating! It involves analyzing text data to extract meaningful insights.\"\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Stopword Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(\"Tokens after stopword removal:\", filtered_tokens)\n",
    "\n",
    "# Stemming\n",
    "ps = PorterStemmer()\n",
    "stemmed_tokens = [ps.stem(word) for word in filtered_tokens]\n",
    "print(\"Stemmed tokens:\", stemmed_tokens)\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "print(\"Lemmatized tokens:\", lemmatized_tokens)\n",
    "\n",
    "# Part-of-Speech Tagging (POS)\n",
    "pos_tags = pos_tag(filtered_tokens)\n",
    "print(\"POS tags:\", pos_tags)\n",
    "\n",
    "# Named Entity Recognition (NER)\n",
    "ner_tags = ne_chunk(pos_tags)\n",
    "print(\"NER tags:\", ner_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121f0726",
   "metadata": {},
   "source": [
    "# 2.Machine Learning Algorithms: Implementing models for sentiment classification, such as Support Vector Machines, Naive Bayes, or deep learning architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5502cae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming you have a dataset 'X' containing text data and 'y' containing corresponding labels\n",
    "# Example dataset\n",
    "\n",
    "\n",
    "\n",
    "X = [\"This is a positive review.\", \"This is a negative review.\", \"Another positive review.\"]\n",
    "y = [\"positive\", \"negative\", \"positive\"]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature extraction using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train the Support Vector Machine (SVM) model\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = svm_model.predict(X_test_tfidf)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1cf5b4",
   "metadata": {},
   "source": [
    "This code snippet demonstrates how to train and evaluate a Support Vector Machine (SVM) model for sentiment classification using the TF-IDF feature extraction technique. You can replace the SVM model with other classifiers and experiment with different feature extraction methods to find the best-performing model for your sentiment analysis task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67448549",
   "metadata": {},
   "source": [
    "# 3.Feature Engineering: Identifying and extracting relevant features from text data to enhance model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6863330e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Example dataset (replace with your actual dataset)\n",
    "texts = [\"This is a positive review.\", \"This is a negative review.\", \"Another positive review.\"]\n",
    "labels = [\"positive\", \"negative\", \"positive\"]\n",
    "\n",
    "# Step 1: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Feature engineering - Bag-of-Words (BoW)\n",
    "# Initialize the CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_bow = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data\n",
    "X_test_bow = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Step 3: Train a Support Vector Machine (SVM) classifier\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train_bow, y_train)\n",
    "\n",
    "# Step 4: Evaluate the model\n",
    "y_pred = svm_model.predict(X_test_bow)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 5: Feature engineering - TF-IDF\n",
    "# Initialize the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Step 6: Retrain the SVM classifier using TF-IDF features\n",
    "svm_model_tfidf = SVC(kernel='linear')\n",
    "svm_model_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Step 7: Evaluate the model with TF-IDF f\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5307755",
   "metadata": {},
   "source": [
    "In this code:\n",
    "\n",
    "We first split the dataset into training and testing sets.\n",
    "\n",
    "We perform feature engineering using Bag-of-Words (BoW) representation and train a Support Vector Machine (SVM) classifier.\n",
    "Then, we evaluate the model's performance.\n",
    "\n",
    "Next, we perform feature engineering using TF-IDF representation and retrain the SVM classifier.\n",
    "\n",
    "Finally, we evaluate the model's performance with TF-IDF features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d9d645",
   "metadata": {},
   "source": [
    "# 4.Data Visualization: Presenting sentiment analysis results through effective visualizations for clear interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61c8aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "def plot_sentiment_distribution(sentiments):\n",
    "    \"\"\"\n",
    "    Plot sentiment distribution using a bar plot.\n",
    "    \"\"\"\n",
    "    sentiment_counts = {sentiment: sentiments.count(sentiment) for sentiment in set(sentiments)}\n",
    "    labels = list(sentiment_counts.keys())\n",
    "    values = list(sentiment_counts.values())\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(labels, values, color=['green', 'blue', 'red'])\n",
    "    plt.xlabel('Sentiment')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Sentiment Distribution')\n",
    "    plt.show()\n",
    "\n",
    "def plot_sentiment_proportion(sentiments):\n",
    "    \"\"\"\n",
    "    Plot sentiment proportion using a pie chart.\n",
    "    \"\"\"\n",
    "    sentiment_counts = {sentiment: sentiments.count(sentiment) for sentiment in set(sentiments)}\n",
    "    labels = list(sentiment_counts.keys())\n",
    "    values = list(sentiment_counts.values())\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.pie(values, labels=labels, autopct='%1.1f%%', colors=['green', 'blue', 'red'])\n",
    "    plt.title('Sentiment Proportion')\n",
    "    plt.show()\n",
    "\n",
    "def generate_wordcloud(text):\n",
    "    \"\"\"\n",
    "    Generate a word cloud from the text data.\n",
    "    \"\"\"\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "sentiments = ['positive', 'negative', 'positive', 'neutral', 'positive', 'negative', 'negative', 'neutral']\n",
    "text = \"This is a positive review. The movie was great and the acting was superb.\"\n",
    "\n",
    "# Plot sentiment distribution\n",
    "plot_sentiment_distribution(sentiments)\n",
    "\n",
    "# Plot sentiment proportion\n",
    "plot_sentiment_proportion(sentiments)\n",
    "\n",
    "# Generate word cloud\n",
    "generate_wordcloud(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48999d3",
   "metadata": {},
   "source": [
    "in this code:\n",
    "\n",
    "The plot_sentiment_distribution function creates a bar plot to visualize the distribution of sentiments in the data.\n",
    "\n",
    "The plot_sentiment_proportion function creates a pie chart to show the proportion of each sentiment category.\n",
    "\n",
    "The generate_wordcloud function generates a word cloud to visualize frequently occurring words in the text data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb88f71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
